# ğŸ§  System 2 Reasoning AI
**Experiments on deliberate, interpretable reasoning across neuro-symbolic models, symbolic regression, inference-time scaling, LLM agents, RL post-training in LLMs, and graph-based retrieval.**

---

## ğŸ§© Overview
This repository gathers six advanced projects exploring how modern AI systems can perform **SystemÂ 2â€“style reasoning** â€” deliberate, structured, and interpretable decisionâ€‘making that combines neural and symbolic approaches. Each notebook represents a distinct yet complementary direction toward **explainable, compositional, and multiâ€‘step reasoning**.

The work spans topics from **neuroâ€‘symbolic program induction** and **symbolic regression** to **inferenceâ€‘time reasoning optimization**, **reinforcement learning postâ€‘training for LLMs**, **visionâ€‘based LLM agents**, and **GraphRAGâ€‘style retrieval**.

---

## ğŸ“š Contents

| # | Project | Path | Description |
|---|--------|------|-------------|
| 1 | **Neuroâ€‘Symbolic Reasoning** | `neuro_symbolic_reasoning/neuro_symbolic_reasoning.ipynb` | CLEVR question answering via program induction, symbolic execution, and seq2seq models (LSTM + Transformer). |
| 2 | **Symbolic Regression** | `symbolic_regression/symbolic_regression.ipynb` | Equation discovery with Equation Learner (EQL) layers and a Transformer Seq2Seq model (token vocabulary â†’ SymPy expressions). |
| 3 | **Inferenceâ€‘Time Scaling** | `inference_time_scaling/inference_time_scaling.ipynb` | Compares Chainâ€‘ofâ€‘Thought, Bestâ€‘ofâ€‘N, Beam Search, Selfâ€‘Refine, Treeâ€‘ofâ€‘Thoughts, A*, and MCTS on math reasoning. |
| 4 | **RL Postâ€‘Training for LLMs** | `rl_post_training/rl_post_training.ipynb` | Twoâ€‘stage fineâ€‘tuning (SFT â†’ GRPO/TRL) with custom rewards for structure (`<think>â€¦</think>` & `<answer>â€¦</answer>`) and correctness. |
| 5 | **Vision LLM Agent** | `vision_llm_agent/vision_llm_agent.ipynb` | Multiâ€‘agent vision reasoning combining OpenCV heuristics with a VLM (Qwenâ€‘VL). Includes ablations and a small 100â€‘image dataset. |
| 6 | **GraphRAG Pipeline** | `graph_rag/graph_rag.ipynb` | Graphâ€‘based retrieval and community reasoning with entity extraction, Leiden community detection, and communityâ€‘scoped answering. |

---

## ğŸ¯ Core Themes
- **SystemÂ 2 Reasoning:** deliberate, multiâ€‘step problem solving  
- **Hybrid Neuroâ€‘Symbolic Learning:** combining neural inference with symbolic structure  
- **Reasoningâ€‘Time Optimization:** inferenceâ€‘time scaling, beam search, and ToT/MCTS exploration  
- **Reinforcement Learning Postâ€‘Training:** reward shaping for structured reasoning in LLMs  
- **LLM Agents & Vision Integration:** combining perception with reasoning chains  
- **Graphâ€‘Based Retrieval:** leveraging community structure for contextual memory  

---

## âš™ï¸ Repository Structure
```
system2-reasoning-ai/
â”‚
â”œâ”€â”€ neuro_symbolic_reasoning/
â”‚   â””â”€â”€ neuro_symbolic_reasoning.ipynb
â”œâ”€â”€ symbolic_regression/
â”‚   â””â”€â”€ symbolic_regression.ipynb
â”œâ”€â”€ inference_time_scaling/
â”‚   â””â”€â”€ inference_time_scaling.ipynb
â”œâ”€â”€ rl_post_training/
â”‚   â””â”€â”€ rl_post_training.ipynb
â”œâ”€â”€ vision_llm_agent/
â”‚   â”œâ”€â”€ vision_llm_agent.ipynb
â”‚   â””â”€â”€ agent_data/
â”‚       â”œâ”€â”€ data.csv
â”‚       â””â”€â”€ images/
â”‚           â””â”€â”€ [100 PNG files]
â””â”€â”€ graph_rag/
    â””â”€â”€ graph_rag.ipynb
```

Each folder contains an independent Jupyter notebook and (where applicable) the supporting data included in your upload.

---

## ğŸ§° Environment Setup
> These notebooks target PythonÂ 3.10+. Install only what you need for the notebook you plan to run.

```bash
pip install torch transformers accelerate datasets tqdm numpy matplotlib pandas sympy scikit-learn             trl peft vllm opencv-python pillow             langchain langchain-community langchain-graphrag networkx cdlib pypdf
```
**Notes**
- Some parts (e.g., RL postâ€‘training and GraphRAG community detection) benefit from a **GPU** runtime.  
- If using verifier or external API calls in the inferenceâ€‘time scaling notebook, configure your **API keys via environment variables** instead of hardâ€‘coding.

---

## ğŸ“Š Selected Observations
- **Vision LLM Agent:** In the provided ablations, the best deepâ€‘agent configuration (AgentÂ 1 + AgentÂ 3) outperformed zeroâ€‘shot and classic pipelines on the 100â€‘image set.  
- **Inferenceâ€‘Time Scaling:** Searchâ€‘based and verificationâ€‘based strategies (e.g., Bestâ€‘ofâ€‘N, ToT/MCTS) showed higher accuracy than plain CoT at additional compute cost.  
- **RL Postâ€‘Training for LLMs:** GRPO with structured rewards improved format compliance and answer correctness relative to SFTâ€‘only baselines.  
- **GraphRAG:** Community summaries derived from the entityâ€‘relation graph improved longâ€‘document question answering quality relative to naive chunk retrieval.

*(Exact metrics depend on runtime settings and hardware; see notebook outputs for details.)*

---

## ğŸ§  Motivation
SystemÂ 1 reasoning in LLMs is fast but often shallow. This project explores **SystemÂ 2 reasoning** â€” deliberate, symbolic, and interpretable â€” by experimenting with architectures and training strategies that encourage models to reason, plan, and reflect.

---

## ğŸª„ How to Mention This Work
If you reference this repository in your CV or portfolio:
> *â€œDeveloped the `system2-reasoning-ai` repository â€” a collection of experiments integrating neuroâ€‘symbolic learning, inferenceâ€‘time scaling, reinforcement learning postâ€‘training in LLMs, LLM agents, and graphâ€‘based retrieval for advanced reasoning.â€*

---

## ğŸ·ï¸ Suggested GitHub Topics
`system2-reasoning`Â Â `neurosymbolic-ai`Â Â `symbolic-regression`Â Â `inference-time-scaling`Â Â `llm-agents`Â Â `reinforcement-learning`Â Â `graph-rag`Â Â `explainable-ai`

---

## ğŸ“œ License
Released for **educational and research purposes**. Please credit the author if you reuse substantial portions of code or results.
